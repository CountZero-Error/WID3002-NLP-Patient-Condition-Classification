{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk download\n",
    "nltk.download('punkt') # token\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "\n",
    "# Just for print information\n",
    "def quick_view(lst, num):\n",
    "    print(f'[*] Quick view(total {num}):')\n",
    "    for i in range(10):\n",
    "        print(f'\\t{i + 1}.{lst[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "fi = 'NLP_Data/drugsComTrain_raw.csv'\n",
    "df = pd.read_csv(fi)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop date\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Drop uniqueID\n",
    "df.drop('uniqueID', axis=1, inplace=True)\n",
    "\n",
    "# Drop all na\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Text Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing text\n",
    "for column in df.columns:\n",
    "    # Lowercasing just 'object' Dtype\n",
    "    if df[column].dtype == 'object':\n",
    "        print(f'[*] Lowercasing {column}...')\n",
    "        df[column] = df[column].str.lower()\n",
    "    \n",
    "    else:\n",
    "        print(f'[*] {df[column].dtype} - {column}: PASS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove everything except alphabetic and number\n",
    "# Output: clean_texts -> list()\n",
    "pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "\n",
    "clean_texts = []\n",
    "for i in df.index:\n",
    "    print(f'\\r[*] Progress: {round(round((i + 1)/df.shape[0], 3)*100, 2)}%', end='')\n",
    "    clean_text = re.sub(pattern, ' ', df.loc[i, 'review'])\n",
    "    clean_texts.append(clean_text)\n",
    "\n",
    "clean_texts_num = len(clean_texts)\n",
    "print('\\n[*] Done.')\n",
    "quick_view(clean_texts, clean_texts_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from clean_texts base on nltk library\n",
    "# Output: clean_texts -> list()\n",
    "\n",
    "# Stop words table (from nltk)\n",
    "stop_wds = stopwords.words('english') \n",
    "\n",
    "tmp = []\n",
    "for i in range(clean_texts_num):\n",
    "    print(f'\\r[*] Progress: {round(round((i + 1)/clean_texts_num, 3)*100, 2)}%', end='')\n",
    "    \n",
    "    clean_text = ' '.join([wd for wd in clean_texts[i].split(' ') if wd not in stop_wds])\n",
    "    tmp.append(clean_text)\n",
    "\n",
    "clean_texts = tmp[:]\n",
    "tmp = []\n",
    "\n",
    "clean_texts_num = len(clean_texts)\n",
    "print('\\n[*] Done.')\n",
    "quick_view(clean_texts, clean_texts_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying important entities and save them to entities_catalog.csv\n",
    "# Output: entities_catalog -> dict()\n",
    "\n",
    "# Entities table (from spacy)\n",
    "recognition = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "entities_catalog = {}\n",
    "for i in range(clean_texts_num):\n",
    "    print(f'\\r[*] Progress: {round(round((i + 1)/clean_texts_num, 3)*100, 2)}%', end='')\n",
    "    \n",
    "    doc = recognition(clean_texts[i]) # recognize core entities\n",
    "    for ent in doc.ents:\n",
    "        entities_catalog.setdefault(ent.label_, [])\n",
    "        entities_catalog[ent.label_].append(ent.text)\n",
    "\n",
    "entities_catalog_num = len(entities_catalog.keys())\n",
    "print('\\n[*] Done.')\n",
    "print(f'[*] Quick view(total {entities_catalog_num}):')\n",
    "for i in range(10):\n",
    "    curr_k = list(entities_catalog.keys())[i]\n",
    "    print(f'\\t{i + 1}.{curr_k} - {entities_catalog[curr_k][1:10]}')\n",
    "\n",
    "fo_path = os.path.join(os.path.dirname(fi), 'entities_catalog.csv')\n",
    "print(f'[*] Generating entities_catalog.csv...')\n",
    "entities_catalog_df = pd.DataFrame({'entity name': entities_catalog.keys(), 'entity contents': entities_catalog.values()})\n",
    "entities_catalog_df.to_csv(fo_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "# Output: tokens -> list()\n",
    "tokens = []\n",
    "\n",
    "for i in range(clean_texts_num):\n",
    "    print(f'\\r[*] Progress: {round(round((i + 1)/clean_texts_num, 3)*100, 2)}%', end='')\n",
    "    \n",
    "    token_wd = word_tokenize(clean_texts[i])\n",
    "    tokens.append(token_wd)\n",
    "\n",
    "tokens_num = len(tokens)\n",
    "print('\\n[*] Done.')\n",
    "print(f'[*] Quick view(total {tokens_num}):')\n",
    "for i in range(10):\n",
    "    print(f'\\t{i + 1}.{tokens[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization with nltk\n",
    "# Output: tokens -> list()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tmp = []\n",
    "for i in range(tokens_num):\n",
    "    print(f'\\r[*] Progress: {round(round((i + 1)/tokens_num, 3)*100, 2)}%', end='')\n",
    "\n",
    "    lemmatized_token = [stemmer.stem(wd) for wd in tokens[i]]\n",
    "    tmp.append(lemmatized_token)\n",
    "\n",
    "lemmatized_tokens = tmp[:]\n",
    "tmp = []\n",
    "\n",
    "tokens_num = len(lemmatized_tokens)\n",
    "print('\\n[*] Done.')\n",
    "quick_view(lemmatized_tokens, tokens_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Token Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all non-alphabetic word\n",
    "# Output: alpha_tokens -> list()\n",
    "alpha_tokens = []\n",
    "for i in range(tokens_num):\n",
    "    print(f'\\r[*] Progress: {round(round((i + 1)/tokens_num, 3)*100, 2)}%', end='')\n",
    "    \n",
    "    alpha_tokens.append([wd for wd in lemmatized_tokens[i] if wd.isalpha()])\n",
    "\n",
    "tokens_num = len(alpha_tokens)\n",
    "print('\\n[*] Done.')\n",
    "quick_view(alpha_tokens, tokens_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data\n",
    "# clean_texts -> list()\n",
    "# tokens -> list()\n",
    "# alpha_tokens -> list()\n",
    "\n",
    "df.insert(df.shape[1], 'clean_texts', clean_texts, allow_duplicates=True)\n",
    "df.insert(df.shape[1], 'tokens', tokens, allow_duplicates=True)\n",
    "df.insert(df.shape[1], 'alpha_tokens', alpha_tokens, allow_duplicates=True)\n",
    "\n",
    "fo_path = os.path.join(os.path.dirname(fi), f'preprocessed_{os.path.basename(fi)}')\n",
    "df.to_csv(fo_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
